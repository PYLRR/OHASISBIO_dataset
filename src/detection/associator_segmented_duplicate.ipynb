{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T17:11:06.853809Z",
     "start_time": "2024-10-16T17:10:55.252359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import itertools\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import glob2\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "from skimage.transform import resize\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from src.detection.associator_segmenter import best_matchups_combinatory, locate, find_candidates\n",
    "from src.detection.clustering import get_valid, get_clusters, get_delta\n",
    "from utils.data_reading.catalogs.ISC import ISC_file\n",
    "from utils.data_reading.sound_data.sound_file_manager import NpyFilesManager\n",
    "from utils.data_reading.sound_data.station import StationsCatalog\n",
    "from utils.physics.bathymetry.bathymetry_grid import BathymetryGrid\n",
    "from utils.physics.sound.sound_model import HomogeneousSoundModel\n",
    "from utils.physics.sound.sound_velocity_grid import MonthlySoundVelocityGridOptimized\n",
    "from utils.transformations.features_extractor import STFTFeaturesExtractor"
   ],
   "id": "46fd83a0a52c0f95",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 19:10:58.076663: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-16 19:10:58.182248: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-16 19:10:58.682238: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-16 19:10:58.682470: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-16 19:10:58.764232: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-16 19:10:58.960327: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-16 19:10:58.963273: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-16 19:11:01.638090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/plerolland/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-16T17:12:01.398667Z",
     "start_time": "2024-10-16T17:11:06.903642Z"
    }
   },
   "source": [
    "datasets_yaml = \"/home/plerolland/Bureau/dataset.yaml\"\n",
    "\n",
    "year = 2022\n",
    "isc_file = f\"/home/plerolland/Bureau/catalogs/ISC/eqk_isc_{year}.txt\"\n",
    "detections_file = f\"../../data/detections/{year}/detections.npy\"  # obtained from associator_preprocess\n",
    "\n",
    "sound_model_h = HomogeneousSoundModel()\n",
    "sound_model_g = MonthlySoundVelocityGridOptimized([f\"../../data/sound_model/min-velocities_month-{i:02d}.nc\" for i in range(1,13)], interpolate=True)\n",
    "if 'bathy_model' not in locals():\n",
    "    bathy_model = BathymetryGrid.create_from_NetCDF(\"../../data/geo/GEBCO_2023_sub_ice_topo.nc\", lat_bounds=[-75, 35], lon_bounds=[-20, 180])\n",
    "\n",
    "stations_c = StationsCatalog(datasets_yaml).filter_out_undated().filter_out_unlocated()\n",
    "stations_c = stations_c.starts_before(datetime.datetime(year+1, 1, 1))\n",
    "stations_c = stations_c.ends_after(datetime.datetime(year, 1, 1))\n",
    "stft_computer = STFTFeaturesExtractor(None, vmin=60, vmax=140, f_min=5, f_max=60)\n",
    "\n",
    "MIN_P = 0.2\n",
    "MIN_P_MATES = 0.2\n",
    "NB = 3\n",
    "TOLERANCE = datetime.timedelta(seconds=20)\n",
    "TIME_DELTA_SEARCH = datetime.timedelta(seconds=5*86400)\n",
    "\n",
    "TIME_RES = 0.5 # duration, in s, of a sample in the associator results\n",
    "HALF_FOCUS_SIZE = 16  # window to compare for associator -> keep what was used for training\n",
    "MAX_SHIFT = 128  # max allowed shift resulting from associator\n",
    "\n",
    "# output\n",
    "RES_FILE = f\"../../data/detections/{year}/matchups_clusters_loc_adjusted_experiment.csv\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Detections computation",
   "id": "88224664cbbfd7d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T17:12:30.180058Z",
     "start_time": "2024-10-16T17:12:01.527225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "detections = np.load(detections_file, allow_pickle=True).item()\n",
    "embedding_managers = {s : NpyFilesManager(f\"../../data/detections/{year}/embedding_{year}_{s.name}-{s.date_start.year}\", fill_empty_with_zeroes=False) for s in detections.keys()}\n",
    "\n",
    "# merge all detections and sort them by date\n",
    "stations = list(detections.keys())\n",
    "merged_detections = []\n",
    "for s, dets in detections.items():\n",
    "    for det in dets:\n",
    "        merged_detections.append((det[0], det[1], s))\n",
    "merged_detections = np.array(merged_detections, dtype=np.object_)\n",
    "merged_detections = merged_detections[np.argsort(merged_detections[:,0])]\n",
    "\n",
    "merged_detections_kept = merged_detections[merged_detections[:,1]>MIN_P]\n",
    "print(f\"{len(merged_detections_kept)} detections kept out of {len(merged_detections)}\")"
   ],
   "id": "60fd086cd7d6948",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100690 detections kept out of 2709809\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ISC loading",
   "id": "38d01cead221f0ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T17:12:32.061907Z",
     "start_time": "2024-10-16T17:12:30.233841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "isc = ISC_file(isc_file)\n",
    "\n",
    "# filter isc events to keep only events from ridges\n",
    "to_del = set()\n",
    "for ID, event in isc.items.items():\n",
    "    if bathy_model.get_nearest_values(event.get_pos()) > 0:\n",
    "        to_del.add(ID)\n",
    "    lat, lon = tuple(isc[ID].get_pos())\n",
    "    if (lat>-5 and lon > 115) or (lat>-30 and lon > 130) or (lat>-50 and lon>150) or (lat>-20 and lon>85):\n",
    "        to_del.add(ID)\n",
    "for ID in to_del:\n",
    "    del isc.items[ID]\n",
    "print(f\"{len(to_del)} terrestrial events removed from catalog ({len(isc.items)} remain)\")\n",
    "\n",
    "IDs = list(isc.items.keys())\n",
    "\n",
    "# ISC clustering\n",
    "allowed_delta = 30\n",
    "delta = get_delta(isc, allowed_delta)\n",
    "valid = get_valid(allowed_delta, delta, IDs)\n",
    "clusters = get_clusters(IDs, valid)\n",
    "\n",
    "res = \"\"\n",
    "for id, cluster in clusters.items():\n",
    "    for ID in cluster:\n",
    "        res += f\"{isc[ID].get_pos()[0]},{isc[ID].get_pos()[1]},{id}\\n\"\n",
    "with open(\"res.csv\", \"w\") as f:\n",
    "    f.write(res)\n",
    "    \n",
    "date_min = [np.min([isc[ID].date for ID in cluster]) for cluster in clusters.values()]\n",
    "clusters = list(clusters.values())\n",
    "clusters = [clusters[i] for i in np.argsort(date_min)]"
   ],
   "id": "5cee0b364d4ef31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13953 terrestrial events removed from catalog (1026 remain)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Association",
   "id": "52cfaf92c1f07786"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T17:12:35.370933Z",
     "start_time": "2024-10-16T17:12:32.156381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def associate_cluster(cluster, stations_c, isc, merged_detections_kept, embedding_managers, res_file, nb=3, date_seen_delta=datetime.timedelta(seconds=20)):\n",
    "    centroid = np.mean([isc[ID].get_pos() for ID in cluster], axis=0)\n",
    "    dates = [isc[ID].date for ID in cluster]\n",
    "    date_min, date_max = np.min(dates)-datetime.timedelta(seconds=86400*1), np.max(dates)+datetime.timedelta(seconds=86400*1)\n",
    "    date_mid = date_min + (date_max - date_min) / 2\n",
    "\n",
    "    stations_crisis = stations_c.ends_after(date_min).starts_before(date_max)\n",
    "    bathy = {s: bathy_model.get_along_path_nearest(centroid, s.get_pos())[0] for s in stations_crisis}\n",
    "    stations_crisis = [s for s in bathy.keys() if np.max(bathy[s])<400]\n",
    "    expected = {s: sound_model_g.get_sound_travel_time(centroid, s.get_pos(), date=date_mid) for s in stations_crisis}\n",
    "    # d_h[s1][s2] = given a detection on s1, time to wait before getting it on s2 (can be negative)\n",
    "    d_h = {s1: {s2: datetime.timedelta(seconds=expected[s2] - expected[s1]) for s2 in stations_crisis} for s1 in\n",
    "           stations_crisis}\n",
    "    associate_area(d_h, merged_detections_kept, embedding_managers, date_min, date_max, res_file, nb, date_seen_delta, initial_pos_loc=centroid, ref_name=cluster[0])\n",
    "\n",
    "def associate_area(d_h, merged_detections_kept, embedding_managers, date_min, date_max, res_file, nb=3, date_seen_delta=datetime.timedelta(seconds=20),\n",
    "              seen_dates=None, initial_pos_loc=None, ref_name=None):\n",
    "    if seen_dates is None:\n",
    "        seen_dates = []\n",
    "    if len(d_h) <= nb:\n",
    "        return\n",
    "    idx_min = np.searchsorted(merged_detections_kept[:, 0], date_min, side='right')\n",
    "    idx_max = np.searchsorted(merged_detections_kept[:, 0], date_max, side='left')\n",
    "    if len(d_h) == 0:\n",
    "        return\n",
    "    max_d = np.max([list(d.values()) for d in d_h.values()])\n",
    "    for idx in tqdm(range(idx_min, idx_max), position=0, leave=False):\n",
    "        detection = merged_detections_kept[idx]\n",
    "        \n",
    "        # update seen dates to keep only recent\n",
    "        to_del = list()\n",
    "        for i in range(len(seen_dates)):\n",
    "            if seen_dates[i] < detection[0] - max_d:\n",
    "                to_del.append(i)\n",
    "        seen_dates = list(np.delete(seen_dates, to_del, axis=0))\n",
    "        if len(seen_dates) > 0 and np.min(detection[0]-np.array(seen_dates)) < date_seen_delta:\n",
    "            continue\n",
    "            \n",
    "        associate(detection, d_h, embedding_managers, res_file, nb, date_seen_delta, seen_dates, initial_pos_loc=initial_pos_loc, ref_name=ref_name, ref_date=date_min)\n",
    "        \n",
    "        \n",
    "def associate(detection, d_h, embedding_managers, res_file, nb=3, date_seen_delta=datetime.timedelta(seconds=20), seen_dates=None, initial_pos_loc=None, ref_name=None, ref_date=None):\n",
    "    if seen_dates is None:\n",
    "        seen_dates = []\n",
    "    candidates = find_candidates(detection, d_h, TOLERANCE, embedding_managers, min_p=MIN_P_MATES)\n",
    "    d1, s1 = detection[0], detection[-1]\n",
    "    \n",
    "    # remove already used dates\n",
    "    s2_to_del = list()\n",
    "    for s2 in candidates.keys():\n",
    "        to_del = list()\n",
    "        for j, candidate in enumerate(candidates[s2]):\n",
    "            if len(seen_dates) > 0 and np.min(candidate[0]-np.array(seen_dates)) < date_seen_delta:\n",
    "                to_del.append(j)\n",
    "        candidates[s2] = np.delete(candidates[s2], to_del, axis=0)\n",
    "        if len(candidates[s2]) == 0:\n",
    "            s2_to_del.append(s2)\n",
    "    for s2 in s2_to_del:\n",
    "        del candidates[s2]\n",
    "        \n",
    "    # check we have enough stations\n",
    "    if len(candidates) < nb:\n",
    "        return\n",
    "        \n",
    "    # get best matchups\n",
    "    scores = {s: c for s, c in candidates.items()}\n",
    "    best_matchups = best_matchups_combinatory(scores, nb, d_h, TOLERANCE)\n",
    "    if len(best_matchups) == 0:\n",
    "        return\n",
    "    matchup_scores = []\n",
    "    for matchup in best_matchups:\n",
    "        matchup = [[d1, 0, s1]] + [[d2, j2, s2] for (d2, j2, s2) in matchup]\n",
    "        loc_worked, loc_res = locate(matchup, sound_model_h, 10, initial_pos=initial_pos_loc)\n",
    "        matchup_scores.append(loc_res.cost if loc_res else np.inf)\n",
    "    best_matchup = best_matchups[np.argmin(matchup_scores)]\n",
    "\n",
    "    matchup = [[d1, 0, s1]] + [[d2, j2, s2] for (d2, j2, s2) in best_matchup]\n",
    "    loc_worked, loc_res = locate(matchup, sound_model_h, 10, initial_pos=initial_pos_loc)\n",
    "\n",
    "    # if it didn't work because loc was not close enough, we try deleting a station\n",
    "    if not loc_worked and type(loc_res) != list and len(matchup) > nb + 1:\n",
    "        to_del = np.argmax(loc_res.fun)  # index of maximum residual\n",
    "        matchup = matchup[:to_del] + matchup[to_del+1:]\n",
    "        loc_worked, loc_res = locate(matchup, sound_model_h, 10, initial_pos=initial_pos_loc)\n",
    "\n",
    "    # in case it worked, go further and try to locate\n",
    "    if loc_worked:\n",
    "        loc_worked, loc_res = locate(matchup, sound_model_g, 10, initial_pos=initial_pos_loc)\n",
    "        if loc_worked:\n",
    "            # add other stations\n",
    "            seen_stations = set([c[-1] for c in matchup])\n",
    "            for s, c in candidates.items():\n",
    "                if s in seen_stations:\n",
    "                    continue\n",
    "                c = np.array(c)\n",
    "                possible = [True] * len(c)\n",
    "                for (d2, _, s2) in matchup:\n",
    "                    possible = np.logical_and(possible, d2 + d_h[s][s2] - TOLERANCE < c[:,0])\n",
    "                    possible = np.logical_and(possible, c[:,0] < d2 + d_h[s2][s] + TOLERANCE)\n",
    "                if np.count_nonzero(possible) == 0:\n",
    "                    continue\n",
    "                c = c[possible]\n",
    "                for chosen in c[np.argsort(c[:,1])]:\n",
    "                    new_matchup = matchup + [chosen]\n",
    "                    loc_worked_new, loc_res_new = locate(new_matchup, sound_model_g, 10, initial_pos=initial_pos_loc)\n",
    "                    if loc_worked_new:\n",
    "                        seen_stations.add(s)\n",
    "                        loc_worked, loc_res = loc_worked_new, loc_res_new\n",
    "                        matchup = new_matchup\n",
    "                        break\n",
    "\n",
    "            det_times = [c[0] for c in matchup]\n",
    "            date_event = np.min(det_times) + datetime.timedelta(seconds=loc_res.x[0])\n",
    "            try:\n",
    "                J = loc_res.jac\n",
    "                cov = np.linalg.inv(J.T.dot(J))\n",
    "                var = np.sqrt(np.diagonal(cov))\n",
    "            except:\n",
    "                var = [-1, -1, -1]\n",
    "\n",
    "            # note: we register one ISC event from the cluster\n",
    "            to_write = (f'{date_event.strftime(\"%Y%m%d_%H%M%S\")},{loc_res.x[1]:.4f},{loc_res.x[2]:.4f},'\n",
    "                        f'{var[0]:.4f},{var[1]:.4f},{var[2]:.4f},{ref_name},{(date_event-ref_date).total_seconds():.1f}')\n",
    "            for d, _, s in matchup:\n",
    "                to_write += f',{s.name}-{s.date_start.year},{d.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "                seen_dates.append(d)\n",
    "            with open(res_file, \"a\") as f:\n",
    "                f.write(to_write + \"\\n\")\n",
    "\n",
    "\n",
    "cluster = clusters[647]\n",
    "print(isc[cluster[0]])\n",
    "#associate_cluster(cluster, stations_c, isc)\n",
    "#with open(RES_FILE, \"r\") as f:\n",
    "#    lines = [[l.strip() for l in line.split(\",\")] for line in f.readlines()]\n",
    "IDS = []\n",
    "#IDS = set([int(id) for id in np.array(lines)[:,6]]) if len(lines)>0 else []\n",
    "#associate_cluster(cluster, stations_c, isc, merged_detections_kept, embedding_managers, RES_FILE, NB)\n",
    "\n",
    "for cluster in tqdm(clusters):\n",
    "    seen = False\n",
    "    for id in cluster:\n",
    "        if id in IDS:\n",
    "            seen = True\n",
    "    if seen:\n",
    "        continue\n",
    "    associate_cluster(cluster, stations_c, isc, merged_detections_kept, embedding_managers, RES_FILE, NB)"
   ],
   "id": "a70108be928ec377",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISC event 625077273 - (-30.2468,60.9115,-0.0) at 2022-10-20 10:36:38.380000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/831 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83dd58e554e743a7b621a994d1fdfa1c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/4226 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0062c0d0f8b4359a7bd631128341ad7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "<utils.data_reading.sound_data.station.Station object at 0x74cbd2d84430>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 147\u001B[0m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m seen:\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m--> 147\u001B[0m \u001B[43massociate_cluster\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcluster\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstations_c\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43misc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmerged_detections_kept\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mRES_FILE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mNB\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 14\u001B[0m, in \u001B[0;36massociate_cluster\u001B[0;34m(cluster, stations_c, isc, merged_detections_kept, embedding_managers, res_file, nb, date_seen_delta)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# d_h[s1][s2] = given a detection on s1, time to wait before getting it on s2 (can be negative)\u001B[39;00m\n\u001B[1;32m     12\u001B[0m d_h \u001B[38;5;241m=\u001B[39m {s1: {s2: datetime\u001B[38;5;241m.\u001B[39mtimedelta(seconds\u001B[38;5;241m=\u001B[39mexpected[s2] \u001B[38;5;241m-\u001B[39m expected[s1]) \u001B[38;5;28;01mfor\u001B[39;00m s2 \u001B[38;5;129;01min\u001B[39;00m stations_crisis} \u001B[38;5;28;01mfor\u001B[39;00m s1 \u001B[38;5;129;01min\u001B[39;00m\n\u001B[1;32m     13\u001B[0m        stations_crisis}\n\u001B[0;32m---> 14\u001B[0m \u001B[43massociate_area\u001B[49m\u001B[43m(\u001B[49m\u001B[43md_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmerged_detections_kept\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdate_min\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdate_max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mres_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdate_seen_delta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_pos_loc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcentroid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mref_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcluster\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 39\u001B[0m, in \u001B[0;36massociate_area\u001B[0;34m(d_h, merged_detections_kept, embedding_managers, date_min, date_max, res_file, nb, date_seen_delta, seen_dates, initial_pos_loc, ref_name)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(seen_dates) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m np\u001B[38;5;241m.\u001B[39mmin(detection[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39marray(seen_dates)) \u001B[38;5;241m<\u001B[39m date_seen_delta:\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m---> 39\u001B[0m \u001B[43massociate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdetection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mres_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdate_seen_delta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseen_dates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_pos_loc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial_pos_loc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mref_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mref_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mref_date\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_min\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 45\u001B[0m, in \u001B[0;36massociate\u001B[0;34m(detection, d_h, embedding_managers, res_file, nb, date_seen_delta, seen_dates, initial_pos_loc, ref_name, ref_date)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m seen_dates \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     44\u001B[0m     seen_dates \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 45\u001B[0m candidates \u001B[38;5;241m=\u001B[39m \u001B[43mfind_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdetection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTOLERANCE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMIN_P_MATES\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m d1, s1 \u001B[38;5;241m=\u001B[39m detection[\u001B[38;5;241m0\u001B[39m], detection[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# remove already used dates\u001B[39;00m\n",
      "File \u001B[0;32m~/Bureau/OHASISBIO_dataset/src/detection/associator_segmenter.py:61\u001B[0m, in \u001B[0;36mfind_candidates\u001B[0;34m(detection, d_h, tolerance, embedding_managers, min_p)\u001B[0m\n\u001B[1;32m     59\u001B[0m delta \u001B[38;5;241m=\u001B[39m tolerance\n\u001B[1;32m     60\u001B[0m max_delta \u001B[38;5;241m=\u001B[39m delta \u001B[38;5;241m+\u001B[39m datetime\u001B[38;5;241m.\u001B[39mtimedelta(seconds\u001B[38;5;241m=\u001B[39mTIME_RES \u001B[38;5;241m*\u001B[39m (MAX_SHIFT \u001B[38;5;241m+\u001B[39m HALF_FOCUS_SIZE))\n\u001B[0;32m---> 61\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m d2 \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m max_delta \u001B[38;5;241m>\u001B[39m \u001B[43membedding_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms2\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mdataset_end \u001B[38;5;129;01mor\u001B[39;00m d2 \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m max_delta \u001B[38;5;241m<\u001B[39m embedding_managers[\n\u001B[1;32m     62\u001B[0m     s2]\u001B[38;5;241m.\u001B[39mdataset_start \u001B[38;5;129;01mor\u001B[39;00m \\\n\u001B[1;32m     63\u001B[0m         d1 \u001B[38;5;241m+\u001B[39m max_delta \u001B[38;5;241m>\u001B[39m embedding_managers[s1]\u001B[38;5;241m.\u001B[39mdataset_end \u001B[38;5;129;01mor\u001B[39;00m d1 \u001B[38;5;241m-\u001B[39m max_delta \u001B[38;5;241m<\u001B[39m embedding_managers[\n\u001B[1;32m     64\u001B[0m     s1]\u001B[38;5;241m.\u001B[39mdataset_start:\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m     66\u001B[0m p, h \u001B[38;5;241m=\u001B[39m get_embedder_similarities(d1, embedding_managers[s1], d2, embedding_managers[s2],\n\u001B[1;32m     67\u001B[0m                                  max_shift\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mround\u001B[39m(delta\u001B[38;5;241m.\u001B[39mtotal_seconds() \u001B[38;5;241m/\u001B[39m TIME_RES), min_p\u001B[38;5;241m=\u001B[39mmin_p)\n",
      "\u001B[0;31mKeyError\u001B[0m: <utils.data_reading.sound_data.station.Station object at 0x74cbd2d84430>"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
