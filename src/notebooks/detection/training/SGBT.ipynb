{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook enables to train the SGBT model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fee8182c89ef7940"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "\n",
    "import glob2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from hyperopt import hp, Trials\n",
    "from hyperopt import fmin, tpe, space_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26b8aced9d5fc1ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/path/to/the/dataset\"  # path where we expect to find directories named \"postives\", \"negatives\" and a csv file\n",
    "CHECKPOINTS_DIR = \"../../../../data/model_saves/SGBT\"  # directory where the model will save its history and checkpoints\n",
    "FOLDS = 5  # number of folds for the cross-validation\n",
    "# search space for SGBT hyperparameters\n",
    "lr_limits = (0.01, 1.0)\n",
    "nb_limits = (1, 10_000)\n",
    "np.random.seed(seed=0)  # seed for RNG"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26d9ed07dab00e62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a03d5e4826fcb45b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(ROOT_DIR + \"/dataset.csv\", \"r\") as f:\n",
    "    csv_reader = csv.reader(f, delimiter=\",\")\n",
    "    lines = list(csv_reader)\n",
    "\n",
    "pos_f = glob2.glob(f\"{ROOT_DIR}/positives/*.npy\")\n",
    "pos_data = {p.split(\"/\")[-1]:np.load(p) for p in pos_f}\n",
    "neg_f = glob2.glob(f\"{ROOT_DIR}/negatives/*.npy\")\n",
    "neg_data = {n.split(\"/\")[-1]:np.load(n) for n in neg_f}\n",
    "\n",
    "posX, negX = [], []\n",
    "for line in lines:\n",
    "    _X_list = posX if line[2]==\"positive\" else negX\n",
    "    data = pos_data if line[2]==\"positive\" else neg_data\n",
    "    station = line[0].split(\"/\")[-1]\n",
    "    idx = int(line[1])\n",
    "    _X_list.append(data[station][idx])\n",
    "posX, negX = np.array(posX), np.array(negX)\n",
    "    \n",
    "print(f\"{len(posX)} positive samples and {len(negX)} negative samples found\")\n",
    "    \n",
    "posY, negY = np.ones(len(posX)), np.zeros(len(negX))\n",
    "posX, posY = shuffle(posX, posY)\n",
    "negX, negY = shuffle(negX, negY)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e681ebe99c647dcd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Make the datasets for a cross-validation approach"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a69e2fc165c180d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_trains, Y_trains, X_valids, Y_valids = [], [], [], []\n",
    "for i in range(FOLDS):\n",
    "    start_valid_idx = int(len(posX) * i / FOLDS)\n",
    "    end_valid_idx = int(len(posX) * (i + 1) / FOLDS)\n",
    "    # unbalanced training set\n",
    "    X_trains.append(np.concatenate((posX[:start_valid_idx], posX[end_valid_idx:],\n",
    "                                    negX[:start_valid_idx], negX[end_valid_idx:])))\n",
    "    Y_trains.append(np.concatenate((posY[:start_valid_idx], posY[end_valid_idx:],\n",
    "                                    negY[:start_valid_idx], negY[end_valid_idx:])))\n",
    "    # balanced validation set\n",
    "    X_valids.append(np.concatenate((posX[start_valid_idx:end_valid_idx],\n",
    "                                    negX[start_valid_idx:end_valid_idx])))\n",
    "    Y_valids.append(np.concatenate((posY[start_valid_idx:end_valid_idx],\n",
    "                                    negY[start_valid_idx:end_valid_idx])))\n",
    "    X_trains[-1], Y_trains[-1] = shuffle(X_trains[-1], Y_trains[-1])\n",
    "    X_valids[-1], Y_valids[-1] = shuffle(X_valids[-1], Y_valids[-1])\n",
    "X_train_all = np.concatenate((posX, negX))\n",
    "Y_train_all = np.concatenate((posY, negY))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40784c5919eab724"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training : hyperparameters tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8310bd0eba36f0d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# loss function for hyperparameters evaluation : we train a classifier on each FOLD and average the AuC.\n",
    "def objective(args):\n",
    "    nb, lr = args\n",
    "    nb = int(nb)\n",
    "    val_score = 0\n",
    "    for i in range(FOLDS):\n",
    "        classifier = HistGradientBoostingClassifier(learning_rate=lr, max_iter=nb, max_depth=4, random_state=0)\n",
    "        res = classifier.fit(X_trains[i], Y_trains[i])\n",
    "        pred = res.predict_proba(X_valids[i])[:,1]\n",
    "        val_score += metrics.roc_auc_score(Y_valids[i], pred)\n",
    "    return 1 - val_score / FOLDS\n",
    "\n",
    "# object to record the values tried by hyperopt\n",
    "trials = Trials()\n",
    "\n",
    "# a priori distributions\n",
    "space = [hp.uniform('nb', nb_limits[0], nb_limits[1]), hp.loguniform('lr', np.log(lr_limits[0]), np.log(lr_limits[1]))]  \n",
    "\n",
    "# minimize the objective over the space\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "best_params = space_eval(space, best)\n",
    "\n",
    "print(best)\n",
    "print(best_params)  # 2421, 0.10524 were obtained\n",
    "print(objective((best_params[0], best_params[1])))\n",
    "\n",
    "# save the history\n",
    "with open(f\"{CHECKPOINTS_DIR}/save_trials_20s\", 'wb') as f:\n",
    "    pickle.dump(trials, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f41584db22fb3f81"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis of the explored space"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "993d5c14d5ebec16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load tried hyperparameters history\n",
    "with open(f\"{CHECKPOINTS_DIR}/save_trials_20s\", 'rb') as f:\n",
    "        trials = pickle.load(f)\n",
    "best = dict.copy(trials.best_trial[\"misc\"][\"vals\"])\n",
    "for i in best.keys():\n",
    "    best[i] = best[i][0]\n",
    "\n",
    "# get tried points\n",
    "xy_HO = [np.array([x['misc']['vals']['lr'] for x in trials.trials]), \n",
    "         np.array([x['misc']['vals']['nb'] for x in trials.trials]),\n",
    "         np.array([x['result']['loss'] for x in trials.trials])]\n",
    "\n",
    "# get best point\n",
    "best_HO = (-trials.best_trial['result']['loss'], (space_eval(space, best)[1], \n",
    "                                                space_eval(space, best)[0]))\n",
    "\n",
    "# drawing mesh to draw the points tried\n",
    "xy_mesh = np.meshgrid(np.linspace(*lr_limits, 2001), np.linspace(*nb_limits, 2001))\n",
    "fct = lambda x, y: np.zeros((len(x),len(y)))\n",
    "fct_mesh = fct(xy_mesh[0], xy_mesh[1])\n",
    "\n",
    "fig0 = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# the color depends on the loss\n",
    "colors = 1 - xy_HO[2]\n",
    "plot = plt.scatter(xy_HO[0][:,0], xy_HO[1][:,0], linewidth=0, marker='.', c=colors)\n",
    "\n",
    "# mark the best result with a cross (which means two lines, one vertical, one horizontal)\n",
    "plt.plot(lr_limits, [best_HO[1][1]]*2, linewidth=1, linestyle='--', color='red')\n",
    "plt.plot([best_HO[1][0]]*2, nb_limits, linewidth=1, linestyle='--', color='red')\n",
    "\n",
    "plt.gca().set_xlim(lr_limits)\n",
    "plt.gca().set_ylim(nb_limits)\n",
    "    \n",
    "_ = fig0.colorbar(plot, ax=plt.gca(), fraction=0.05, pad=0.07, aspect=18)\n",
    "plt.xlabel('learning rate', fontsize=12)\n",
    "plt.ylabel('number of trees', fontsize=12)\n",
    "\n",
    "plt.gca().grid(True)\n",
    "plt.gca().set_aspect(lr_limits[1]/nb_limits[1])\n",
    "plt.savefig('../../../figures/SGBT_parameters_space.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "188ff136f4909a3e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final training on all data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "490eddb841faf029"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier = HistGradientBoostingClassifier(learning_rate=0.10524, max_iter=2421, max_depth=4, random_state=0)\n",
    "for i in range(100):\n",
    "    classifier.fit(X_train_all, Y_train_all)\n",
    "\n",
    "with open(f\"{CHECKPOINTS_DIR}/save_model_20s\", 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d9cd4c4d2b4c8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
